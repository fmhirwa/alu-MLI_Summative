# -*- coding: utf-8 -*-
"""Chatbot ALU admissions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-rs16fwnfQPU_rUHPO6ol2qqCeAaK2rq

**ALU Admissions CHatbot**

Steps
1. Tokenization
2. Fine tuning GPT 2 From OpenAI
3. Evaluation against Eval
4. UI with Gradio

1. Tokenization + Preprocessing
"""

# Import necessary classes from tokenizers and transformers
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace
from transformers import PreTrainedTokenizerFast

# Step 1: Initialize the tokenizer with the BPE model and specify the unknown token
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

# Step 2: Create a trainer with special tokens you want to include
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

# Step 3: Set the pre-tokenizer to split text by whitespace
tokenizer.pre_tokenizer = Whitespace()

# Step 4: Define the list of file paths that the tokenizer will be trained on.
files = ["data/file1.txt", "data/ALU_train2.txt", "data/ALU_val3.txt"]

# Step 5: Train the tokenizer on the specified files using the trainer
tokenizer.train(files, trainer)

# The trained tokenizer to a JSON file for future re-use
tokenizer.save("tokenizer.json")

# Step 6: Load the trained tokenizer into the Transformers library as a fast tokenizer
# Directly from the tokenizer object
fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)

# Alternatively, load from the saved JSON file (uncomment the next line if needed)
# fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")

# Encode some sample text using the fast tokenizer
sample_text = "Welcome to African Leadership University."
encoded = fast_tokenizer.encode(sample_text)

# Print the encoded token IDs directly (encoded is a list of token IDs)
print("Encoded IDs:", encoded)


encoded_plus = fast_tokenizer.encode_plus(
    sample_text,
    add_special_tokens=True,
)
print("Encoded IDs using encode_plus:", encoded_plus["input_ids"])

# Print the tokenized output (tokens as strings)
print("Tokenized Output:", fast_tokenizer.tokenize(sample_text))

"""2. Fine-tuning GPT2"""

import os
from transformers import (
    GPT2LMHeadModel,
    GPT2TokenizerFast,
    TextDataset,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments
)

# Define function to load a text dataset for language modeling
def load_dataset(file_path, tokenizer, block_size=128):
    return TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=block_size,
        overwrite_cache=True
    )

# Paths to training and validation text files
train_file = "./data/ALU_train2.txt"  # Contains ALU info & conversational pairs
val_file = "./data/ALU_val3.txt"

# Load the pre-trained GPT-2 model and tokenizer
model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2TokenizerFast.from_pretrained(model_name)

# Use custom tokenizer from token_code.py
#from transformers import PreTrainedTokenizerFast
#okenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")

# Create datasets
train_dataset = load_dataset(train_file, tokenizer, block_size=128)
val_dataset = load_dataset(val_file, tokenizer, block_size=128) if os.path.exists(val_file) else None

# Create a data collator for language modeling (no masking for GPT-2)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Define training hyperparameters using TrainingArguments
training_args = TrainingArguments(
    output_dir="./gpt2-finetuned-alu",
    overwrite_output_dir=True,
    num_train_epochs= 50,                  # number of epochs
    per_device_train_batch_size=2,       # batch size based on hardware
    per_device_eval_batch_size=2,
    evaluation_strategy="steps" if val_dataset else "no",
    eval_steps=500,                      # Evaluation frequency
    logging_steps=100,
    save_steps=500,
    learning_rate=5e-5,
    weight_decay=0.01,
    report_to="none",                    # Set to "tensorboard" or "wandb" if desired
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Start fine-tuning the model
trainer.train()

# Save the fine-tuned model and tokenizer for later use
model.save_pretrained("./gpt2-finetuned-alu")
tokenizer.save_pretrained("./gpt2-finetuned-alu")

"""3. Evaluation"""

import os
import re
import torch
import matplotlib.pyplot as plt
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

# Load fine-tuned GPT-2 model and tokenizer using consistent classes
model_dir = "./gpt2-finetuned-alu"
model = GPT2LMHeadModel.from_pretrained(model_dir)
tokenizer = PreTrainedTokenizerFast.from_pretrained(model_dir)
model.eval()

def compute_f1(prediction, reference):
    pred_tokens = prediction.split()
    ref_tokens = reference.split()
    common = set(pred_tokens) & set(ref_tokens)
    if not common:
        return 0.0
    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(ref_tokens)
    if precision + recall == 0:
        return 0.0
    return 2 * (precision * recall) / (precision + recall)

# Load evaluation prompts and references
with open("./data/ALU_val2.txt", "r", encoding="utf-8") as f:
    eval_data = f.read()

pattern = r"\[Prompt \d+\]\s*Q:\s*(.*?)\s*A:\s*(.*?)\s*(?=\[Prompt \d+\]|$)"
matches = re.findall(pattern, eval_data, re.DOTALL)
prompts, references = zip(*[(m[0].strip(), m[1].strip()) for m in matches])

# Evaluate each prompt
bleu_scores, f1_scores, generated_responses = [], [], []
for prompt, ref in zip(prompts, references):
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    attention_mask = torch.ones_like(input_ids)  # attention_mask added
    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=100,                  # Reduced length for concise answers
            num_beams=7,                     # Increased beams for better quality
            no_repeat_ngram_size=3,          # Higher ngram for reduced repetition
            temperature=0.7,                 # More controlled generation
            top_p=0.9,                       # Nucleus sampling
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    # Remove the prompt if it is repeated
    if generated_text.startswith(prompt):
        generated_text = generated_text[len(prompt):].strip()
    # Limit the generated answer to just one line:
    generated_text = generated_text.split("\n")[0].strip()
    generated_responses.append(generated_text)

    smoothing_fn = SmoothingFunction().method1
    bleu = sentence_bleu([ref.split()], generated_text.split(), smoothing_function=smoothing_fn)
    f1 = compute_f1(generated_text, ref)
    bleu_scores.append(bleu)
    f1_scores.append(f1)

    print(f"Prompt: {prompt}\nReference: {ref}\nGenerated: {generated_text}\nBLEU: {bleu:.4f}, F1: {f1:.4f}\n" + "-"*50)

# Plot BLEU and F1 scores
x = range(1, len(prompts) + 1)
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(x, bleu_scores, color='skyblue')
plt.xlabel("Prompt Number")
plt.ylabel("BLEU Score")
plt.title("BLEU Scores per Prompt")

plt.subplot(1, 2, 2)
plt.bar(x, f1_scores, color='lightgreen')
plt.xlabel("Prompt Number")
plt.ylabel("F1 Score")
plt.title("F1 Scores per Prompt")

plt.tight_layout()
plt.show()

# Summary Metrics
print(f"Average BLEU Score: {sum(bleu_scores)/len(bleu_scores):.4f}")
print(f"Average F1 Score: {sum(f1_scores)/len(f1_scores):.4f}")

!pip install -q gradio

import time
import gradio as gr
import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

# Load the fine-tuned GPT-2 model and tokenizer
model_dir = "./gpt2-finetuned-alu"
model = GPT2LMHeadModel.from_pretrained(model_dir)
tokenizer = GPT2TokenizerFast.from_pretrained(model_dir)
model.eval()

def generate_response(message, history):
    """
    Generate a response from GPT-2 based on user input and chat history.
    1. Simulate a 2-second progress for the user.
    2. Limit the answer to one line.
    3. Return an empty string to clear user input.
    """
    # Simulate a short progress bar (2 seconds)
    time.sleep(2)

    # Combine previous history into context
    context = ""
    for user_msg, bot_msg in history:
        context += f"User: {user_msg}\nAI: {bot_msg}\n"
    context += f"User: {message}\nAI:"

    # Encode the context
    input_ids = tokenizer.encode(context, return_tensors="pt", max_length=1024, truncation=True)
    attention_mask = torch.ones_like(input_ids)

    # Generate response
    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=150,
            num_beams=5,
            no_repeat_ngram_size=2,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # Extract the AI's new response from the generated text
    response = generated_text.split("AI:")[-1].strip()
    # Limit the answer to just one line
    response = response.split("\n")[0].strip()

    # Append the new exchange to history
    history.append((message, response))

    # Return updated history for the chatbot, the same history state,
    # and an empty string to clear the user input
    return history, history, ""

# Updated CSS to spread out the chat bubbles and darken the user bubble
custom_css = """
.chatbox {
    display: flex;
    flex-direction: column;
    height: 600px;
    overflow-y: auto;
    padding: 10px;
    background-color: #f9f9f9;
    border-radius: 10px;
    width: 100%;
}

.user {
    align-self: flex-start;
    background-color: #e2e2e2; /* Darker gray for contrast */
    color: #000000;
    border-radius: 10px;
    padding: 10px;
    margin-bottom: 10px;
    line-height: 1.5;
    width: 75%; /* Increase bubble width */
    margin-right: auto; /* Keep the bubble on the left side */
}

.ai {
    align-self: flex-end;
    background-color: #cfe2ff;
    color: #084298;
    border-radius: 10px;
    padding: 10px;
    margin-bottom: 10px;
    line-height: 1.5;
    width: 75%; /* Increase bubble width */
    margin-left: auto; /* Keep the bubble on the right side */
}
"""

with gr.Blocks(css=custom_css) as iface:
    gr.Markdown("# 🤖 **ALU Chatbot**\nAsk me anything about African Leadership University — Admissions, Programs, or Financial Aid!")

    chatbot = gr.Chatbot(label="Chat History", elem_id="chatbot", height=500)

    with gr.Row():
        user_input = gr.Textbox(
            show_label=False,
            placeholder="How can I help?",
            lines=1,
            scale=10
        )
        send_button = gr.Button("Send", scale=1)

    # State to store conversation history
    state = gr.State([])

    # We now output three values from generate_response:
    #   1) updated chatbot history
    #   2) updated state
    #   3) empty string for clearing user input
    user_input.submit(
        fn=generate_response,
        inputs=[user_input, state],
        outputs=[chatbot, state, user_input]
    )
    send_button.click(
        fn=generate_response,
        inputs=[user_input, state],
        outputs=[chatbot, state, user_input]
    )

iface.launch(share=True)